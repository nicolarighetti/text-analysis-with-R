---
title: "Intermediate techniques in text analysis with R"
format: revealjs
---

# Overview of common text analysis techniques {background-color="#2780e3"}

```{r}
library(tidyverse)
library(tidytext)
library(quanteda)
library(textstem)
library(topicmodels)
library(ldatuning)
library(topicdoc)
library(stm)
library(udpipe)
library(parallel)
library(doParallel)
library(seededlda)
library(BTM)
```

## Text classification and clustering

Classification and clustering both aim at **partitioning** textual data into classes, but the former does that using **supervised** learning methods and the latter via **unsupervised** learning methods.

![The difference between supervised and unsupervised learning, illustrated by \@Ciaraioch](slides-figures/fig0201.png){alt="The difference between supervised and unsupervised learning, illustrated by @Ciaraioch"}

------------------------------------------------------------------------

Supervised learning algorithms perform text classification based on examples (such as textual data labeled by human coders) while unsupervised learning algorithms distinguish among classes based on the patterns detected in the data. Topic modeling is an example of unsupervised learning algorithm, which aims at partitioning the corpus based on patterns of co-occurrences between words.

# Topic modeling {background-color="#2780e3"}

------------------------------------------------------------------------

*"The data-driven and computational nature of LDA makes it attractive for communication research because it allows for quickly and efficiently deriving the thematic structure of large amounts of text documents. It combines an inductive approach with quantitative measurements, making it particularly suitable for exploratory and descriptive analyses"* [^1]

[^1]: *Maier, D., Waldherr, A., Miltner, P., Wiedemann, G., Niekler, A., Keinert, A., \... & Adam, S. (2018). [Applying LDA topic modeling in communication research: Toward a valid and reliable methodology](https://www.tandfonline.com/doi/abs/10.1080/19312458.2018.1430754). Communication Methods and Measures, 12(2-3), 93-118.*

------------------------------------------------------------------------

Topic modeling is one of the most popular unsupervised text analysis techniques in the social sciences. Literature provides critical reviews on the application of topic modeling[^2], and useful suggestions on the process to follow (e.g., the already cited Maier et al., 2018). The most popular approach to topic modeling is **Latent Dirichlet Allocation (LDA)**, which categorizes text within a document into a specific topic. This approach is traced back to Blei, Ng, & Jordan (2003)[^3].

[^2]: Chen, Y., Peng, Z., Kim, S. H., & Choi, C. W. (2023). [What We Can Do and Cannot Do with Topic Modeling: A Systematic Review.](https://www.tandfonline.com/doi/pdf/10.1080/19312458.2023.2167965) *Communication Methods and Measures*, 1-20.

[^3]: Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). [Latent dirichlet allocation](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf?ref=https://githubhelp.com). *Journal of machine Learning research*, *3*(Jan), 993-1022.

------------------------------------------------------------------------

![David M. Blei, "Introduction to Probabilistic Topic Models" (Figure 1)](slides-figures/fig0202.png){alt="David M. Blei, \"Introduction to Probabilistic Topic Models\" (Figure 1)"}

------------------------------------------------------------------------

The "latent" dimension in the Latent Dirichlet Allocation is the hidden structure of the document, which LDA aims to discover: *"(...) the goal of topic modeling is to automatically discover the topics from a collection of documents. The documents themselves are observed, while the topic structure---the topics, per-document topic distributions, and the per-document per-word topic assignments---are hidden structure"* [^4]

[^4]: Blei, D. [Introduction to Probabilistic Topic Models](http://machinelearning202.pbworks.com/f/Blei2011.pdf)

------------------------------------------------------------------------

In a topic model, each document is considered a mixture of topics, and each topic is a mixture of words.

Topic modeling uses the observed documents to infer the hidden topic structure. While the statistical model is sophisticated, there are two important parameters in LDA which can be intuitively understood. They are also called **hyperparameters** and influence the shape and specificity of, respectively, **topic distribution per document** ($\alpha$ parameter) and **term distribution per topic** ($\beta$ parameter).

------------------------------------------------------------------------

Higher values for $\alpha$ lead to a balanced distribution of topics within a document. On the other hand, lower alpha prior values concentrate the probability mass on fewer topics for each document. Similarly, lower values for $\beta$ lead to a balanced distribution of words within a topic. On the other hand, lower values concentrate the probability mass on fewer words for each topic. The hyperparameters *alpha* and *beta* can be estimated by the topic modeling packages automatically.

Moreover, the applied researcher needs to specify the **number of topics** prior to the analysis, which is usually not a trivial choice.

## Introduction to R for topic modeling.

There are several different ways to perform topic modeling analysis in R. A widely used package is [topicmodels](https://cran.r-project.org/web/packages/topicmodels/index.html).

```{r}
ukimmig2010 <- data.frame(text = quanteda::data_char_ukimmig2010)
ukimmig2010$party <- names(quanteda::data_char_ukimmig2010)

ukimmig2010 <- ukimmig2010 %>%
  mutate(lemma = tolower(text)) %>%
  mutate(lemma = lemmatize_strings(lemma))

ukimmig2010_corpus <- quanteda::corpus(ukimmig2010,
                                       docid_field = "party",
                                       text_field = "lemma")

ukimmig2010_dfm <- ukimmig2010_corpus %>%
  quanteda::tokens(remove_punct = TRUE,
         remove_symbols = TRUE,
         remove_numbers = TRUE) %>%
  tokens_tolower() %>%
  tokens_remove(stopwords("en")) %>%
  dfm() %>%
  dfm_trim(min_docfreq = 0.5, max_docfreq = 0.99, 
           docfreq_type = "prop") %>%
  dfm_subset(ntoken(.) > 0)
```

```{r echo=TRUE}
ukimmig2010_dtm <- convert(ukimmig2010_dfm, 
                           to = "topicmodels")
```

### Fit the model

The basic function to fit a topic model is `LDA`:

```{r echo=TRUE}
topicModel <- LDA(ukimmig2010_dtm, 
                  k = 5, 
                  method="Gibbs")
```

------------------------------------------------------------------------

```{r}
topicmodels::terms(topicModel, 5)
```

------------------------------------------------------------------------

Considering the output of the function `LDA`, the `beta` matrix includes the information about the distribution of terms by topics.

```{r}
tidy_model <- tidy(topicModel)

top_terms <- tidy_model %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta)) +
  geom_bar(stat = "identity") +
  scale_x_reordered() +
  facet_wrap(~ topic, scales = "free_x") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) 

```

------------------------------------------------------------------------

### Topic per document

Information about the distribution of topics in each documents is in the matrix `gamma`.

```{r}
tidy_model_gamma <- tidy(topicModel, matrix = "gamma")
tidy_model_gamma
```

------------------------------------------------------------------------

```{r}
tidy_model_gamma <- tidy_model_gamma %>%
  mutate(document = 
           mgsub::mgsub(string = document,
                        pattern = unique(tidy_model_gamma$document),
                        replacement = names(data_char_ukimmig2010),
                        fixed = TRUE))

ggplot(tidy_model_gamma) +
  geom_col(aes(x = topic, y = gamma)) +
  facet_wrap(~ document, nrow = 3)
```

------------------------------------------------------------------------

You may want to assign the most prevalent topic to each document in the corpus.

```{r}
docvars(ukimmig2010_corpus, "pred_topic") <- topicmodels::topics(topicModel)
ukimmig2010_topic_df <- convert(ukimmig2010_corpus, to = "data.frame")
```

### Visualize topics

Beyond plots of topic frequency, there are advanced and interactive options like those provided by `LDAvis`.

```{r eval=FALSE}
# json_lda <- LDAvis::createJSON(
#   phi = posterior(topicModel15)$terms %>% as.matrix, 
#   theta = posterior(topicModel15)$topics %>% as.matrix,
#   vocab = colnames(phi),
#   doc.length = stri_count(ukimmig2010, regex = "\\S+"),
#   term.frequency = colSums(ukimmig2010_dfm))
# 
# LDAvis::serVis(json_lda)
```

# Hands-on tutorial {background-color="#2780e3"}

Click [here](https://nicolarighetti.github.io/text-analysis-with-R/day2-tutorial.html#/sec-introduction-to-r-for-topic-modeling) to open the tutorial

# Coffee Break {background-color="#daffad"}

# Advanced topics in topic modeling {background-color="#2780e3"}

## Validation

Validation is a central problem for scientific analysis, particularly when based on unsupervised techniques. In particular topic models are sensitive at the initial choice of number of topics, hence, the validation of such a number is one of the principal goals of validation.

There are different methods in literature to validate a topic model. While these computational approaches can help identifying a proper model, human intersubjective interpretability remains the most important criterion.

------------------------------------------------------------------------

### Identify the number of topics

While the hyperparameters *alpha* and *beta* can be estimated by the `topicmodels` algorithm, the researchers need to indicate the number of topics. The choice can be informed by the previous knowledge of the researcher about the topic or/and supported by a data driven approach.

------------------------------------------------------------------------

There are different algorithms for estimating the optimal number of topics. An approach is to fit several models and choosing the one with the maximum log-likelihood. The `ldatuning` package provides a function `FindTopicsNumber` that calculates different metrics to estimate the most preferable number of topics for LDA model.

```{r}
tn <- FindTopicsNumber(ukimmig2010_dtm, 
                       topics = seq(5, 50, by = 5),
                       metrics = c("Griffiths2004", 
                                   "CaoJuan2009", 
                                   "Arun2010", 
                                   "Deveaud2014"))
FindTopicsNumber_plot(tn)
```

------------------------------------------------------------------------

Here, the `Griffiths2004` approach is based on the log-likelihood maximization and it is described in the related [paper](https://www.pnas.org/doi/10.1073/pnas.0307752101). It is also the default approach of `ldatuning`. The Deveaud2014 is also a common choice.

```{r}
ggplot(tn,
       aes(x = topics, y = Griffiths2004)) +
  geom_point() +
  geom_line() + 
  ggtitle("Griffiths2004")
```

------------------------------------------------------------------------

Based on this indication, we can peraphs fit a model with about 15 topics.

```{r}
topicModel15 <- LDA(ukimmig2010_dtm, 
                  k = 15, 
                  method="Gibbs")

topicmodels::terms(topicModel15, 5)

tidy_model_gamma_15 <- tidy(topicModel15, matrix = "gamma")
tidy_model_gamma_15 <- tidy_model_gamma_15 %>%
  mutate(document = 
           mgsub::mgsub(string = document,
                        pattern = unique(tidy_model_gamma_15$document),
                        replacement = names(data_char_ukimmig2010),
                        fixed = TRUE))

```

------------------------------------------------------------------------

```{r}
ggplot(tidy_model_gamma_15) +
  geom_col(aes(x = topic, y = gamma)) +
  facet_wrap(~ document, nrow = 3)
```

------------------------------------------------------------------------

### Coherence and exclusivity

The package `topicdoc` provides diagnostic measures for topic models. They can be used to compare different models. Usually, models with a different number of topics are being compared.

A particularly useful and commonly-used metrics are **semantic coherence** and **exclusivity**. A good topic model should have coherent topics (i.e., about a single theme and not a mixture of different themes), which also are well distinguishable from each other, without overlaps (exclusivity).

```{r}
topicModel_diag <- topic_diagnostics(topicModel, ukimmig2010_dtm)
```

------------------------------------------------------------------------

```{r}
topicModel_diag %>%
  mutate(topic = as_factor(topic_num)) %>%
  ggplot() +
  geom_point(aes(x = topic_coherence, y = topic_exclusivity, color = topic),
             size = 3) +
  ylab(label = "Semantic Coherence") +
  xlab("Exclusivity") +
  ggtitle("A topic model with 5 topics")
```

------------------------------------------------------------------------

### Held-out likelihood (perplexity)

**Perplexity** is a metric for the accuracy of a probability model in predicting a sample and can be used as a measure of a topic model's ability to predict new data. **The lower the perplexity, the better the model.**

Topic models with different number of topics can be compared based on perplexity using **cross-validation**. This involves dividing data into subsets (usually 5), and using one subset as the validation set while using the remaining as the training set. This ensures that each data point has an equal opportunity of being part of the validation and training sets.

------------------------------------------------------------------------

```{r echo=FALSE}
cluster <- makeCluster(detectCores(logical = TRUE) - 1) 
registerDoParallel(cluster)

clusterEvalQ(cluster, {
   library(topicmodels)
})

burnin <- 1000
iter <- 1000
keep <- 50

full_data <- ukimmig2010_dtm
n <- nrow(full_data)
folds <- 5
splitfolds <- sample(1:folds, n, replace = TRUE)
candidate_k <- c(2, 5, 7, 10, 15, 20, 50, 100)

clusterExport(cluster, c("full_data", "burnin", "iter", "keep", "splitfolds", "folds", "candidate_k"))

# we parallelize by the different number of topics.  
# A processor is allocated a value of k, and does the cross-validation serially.  This is because it is assumed there are more candidate values of k than there are cross-validation folds, hence it will be more efficient to parallelise
system.time({
results <- foreach(j = 1:length(candidate_k), .combine = rbind) %dopar%{
   k <- candidate_k[j]
   results_1k <- matrix(0, nrow = folds, ncol = 2)
   colnames(results_1k) <- c("k", "perplexity")
   for(i in 1:folds){
      train_set <- full_data[splitfolds != i , ]
      valid_set <- full_data[splitfolds == i, ]
      
      fitted <- LDA(train_set, k = k, method = "Gibbs",
                    control = list(burnin = burnin, iter = iter, keep = keep) )
      results_1k[i,] <- c(k, perplexity(fitted, newdata = valid_set))
   }
   return(results_1k)
}
})
stopCluster(cluster)

results_df <- as.data.frame(results)

```

------------------------------------------------------------------------

```{r}
ggplot(results_df, aes(x = k, y = perplexity)) +
   geom_point() +
   geom_smooth(se = FALSE) +
   ggtitle("5-fold cross-validation") +
   labs(x = "Candidate number of topics", 
        y = "Perplexity when fitting the trained model to the hold-out set")
```

# Hands-on tutorial {background-color="#2780e3"}

Click [here](https://nicolarighetti.github.io/text-analysis-with-R/day2-tutorial.html#/sec-advanced-topic-modeling-methods) to open the tutorial

# Lunch {background-color="#daffad"}

# Advanced topic modeling methods {background-color="#2780e3"}

## Structural Topic Models

The Structural Topic Model is a general framework for topic modeling with **document-level covariate information**. The covariates can improve inference and qualitative interpretability and are allowed to affect topical prevalence, topical content, or both.

The R package [stm](https://www.jstatsoft.org/article/view/v091i02) implements the estimation algorithms for the model and also includes tools for every stage of a standard workflow, from reading in and processing raw text to making publication quality figures.

------------------------------------------------------------------------

[![Roberts, M. E., Stewart, B. M., & Tingley, D. (2019). Stm: An R package for structural topic models. Journal of Statistical Software, 91, 1-40. Chicago](slides-figures/fig0203.png)](https://www.jstatsoft.org/article/view/v091i02)

```{r}
usa_inaugural_df <- read.csv(file = "data/usa_inaugural_df.csv")

# load the udpipe English model
udpipe_english_model <- udpipe_load_model(file = "./english-ewt-ud-2.5-191206.udpipe")

# annotate the text
usa_inaugural_udpipe <- udpipe_annotate(udpipe_english_model, 
                                  x = usa_inaugural_df$text, 
                                  tagger = "default", 
                                  parser = "none") %>%
                  as.data.frame()

inaug_dfm <- usa_inaugural_df %>%
  corpus() %>%
  quanteda::tokens(remove_punct = TRUE,
         remove_symbols = TRUE,
         remove_numbers = TRUE) %>%
  tokens_replace(
    # search for the original token
    pattern = usa_inaugural_udpipe$token, 
    # replace it with its canonical form
    replacement = usa_inaugural_udpipe$lemma,
    # use exact matching
    valuetype = "fixed") %>%
  tokens_tolower() %>%
  tokens_select(pattern = stopwords("en"), selection = "remove") %>%
    dfm() %>%
    dfm_trim(min_docfreq = 0.5, max_docfreq = 0.99, 
           docfreq_type = "prop") %>%
  dfm_subset(ntoken(.) > 0)

```

------------------------------------------------------------------------

### Preprocessing

After the usual pre-processing steps using Quanteda, the resulting Quanteda `dfm` can be transformed to an `stm` object, ready for `prepDocuments` to complete the preparation for the analysis.

```{r echo=TRUE}
inaug_stm_dfm <- convert(inaug_dfm, to = "stm")

out <- prepDocuments(inaug_stm_dfm$documents, 
                     inaug_stm_dfm$vocab, 
                     inaug_stm_dfm$meta)
```

------------------------------------------------------------------------

### Fit

The homonym function `stm` is used to fit the model. Note the covariate that has been added, namely `Year`, for which we are going to fit a smoothed (`s`) function. In this case, the prevalence of topics is allowed to vary over the years. Also, notice the `Spectral` initialization. This has been found an helpful method to estimate the model, whose results can otherwise be very sensitive to initialization.

```{r echo=TRUE}
inaug_stm_fit <- stm(documents = out$documents, 
                     vocab = out$vocab,
                     data = out$meta,
                     K = 9, 
                     prevalence = ~s(Year) + Party, 
                     init.type = "Spectral")
```

------------------------------------------------------------------------

### Estimate effects

It is then possible to analyze the results. In this case, by checking the variation in topic prevalence over time, and by party.

```{r}
estimate_inaug <- estimateEffect(1:9 ~ s(Year) + Party,
                                 inaug_stm_fit,
                                 meta = out$meta,
                                 uncertainty = "Global")
# summary(year_topic, topics = 1)
```

```{r}
par(mfrow=c(1,3))
for(i in 1:3){
  plot(
  estimate_inaug,
  covariate = "Year",
  method = "continuous",
  topics = i,
  model = inaug_stm_fit,
  printlegend = FALSE,
  xlab = "Time")
}
```

------------------------------------------------------------------------

```{r}
par(mfrow = c(2, 3))
for(i in 1:6){
  plot.estimateEffect(
  estimate_inaug,
  covariate = "Party",
  method = "difference",
  cov.value1 = "Democratic",
  cov.value2 = "Republican",
  topics = i,
  model = inaug_stm_fit,
  printlegend = FALSE,
  xlab = "← Republican     Democratic →",
  main = "Democratic vs Republican (D-R)",
  xlim = c(-0.2, 0.2),
  cex = 0.5,
  labeltype = "custom",
  custom.labels = c("", "")
)
}
```

------------------------------------------------------------------------

### Estimates {.scrollable}

The result can be read as a regression model.

```{r}
summary(estimate_inaug)
```

------------------------------------------------------------------------

### Interpretation

We can extract the most representative words per topic, and also extract representative quotes from the text.

```{r}
labelTopics(inaug_stm_fit, 8)
```

------------------------------------------------------------------------

The interpretation can be supported by considering the relationships between topics.

```{r}
inaug_stm_fit_corr <- topicCorr(inaug_stm_fit)
plot(inaug_stm_fit_corr)
```

------------------------------------------------------------------------

### Determine the number of topics

There is not a "right" answer to the number of topics that are appropriate for a given corpus, but the function `searchK` uses a data-driven approach to selecting the number of topics. The function will perform several automated tests to help choose the number of topics.

------------------------------------------------------------------------

```{r}
k_search <- searchK(out$documents, out$vocab, K = c(5, 10, 15, 20),
                    prevalence = ~s(Year) + Party, 
                    data = out$meta, init.type = "Spectral",
                    verbose = FALSE)
```

```{r}
plot.searchK(k_search)
```

------------------------------------------------------------------------

The **held-out likelihood** is similar to cross-validation, when some of the data is removed from estimation and then later used for validation. It helps the user assess the model's prediction performance. The higher, the better.

If **Residuals** are overdispersed, it could be that more topics are needed to soak up some of the extra variance. The lower, the better.

**Semantic coherence** correlates well with human judgment of topic quality. The higher, the better. It is best read along with **Exclusivity**. Indeed, the authors found that *semantic coherence alone is relatively easy to achieve by having only a couple of topics which all are dominated by the most common words. Thus we also proposed an exclusivity measure*,

------------------------------------------------------------------------

```{r}
k_search
```

------------------------------------------------------------------------

```{r}
k_search$results %>%
  select(K, exclus, semcoh) %>%
  mutate(K = unlist(K),
         exclus = unlist(exclus),
         semcoh = unlist(semcoh)) %>%
  mutate(K = as_factor(K)) %>%
  ggplot() +
  geom_point(aes(x = exclus, y = semcoh, color = K), size = 5) +
  ggtitle("Semantic Coherence vs Exclusivity") +
  ylab("Semantic Coherence") +
  xlab("Exclusivity")
```

# Hands-on tutorial {background-color="#2780e3"}

Click [here](https://nicolarighetti.github.io/text-analysis-with-R/day2-tutorial.html#/sec-structural-topic-models) to open the tutorial

## Seeded Topic Models

```{r}
guardian <- read.csv("http://www.luigicurini.com/uploads/6/7/9/8/67985527/guardian.csv")
```

*Seeded LDA (...) is a variant of the standard LDA approach (see Lu et al. 2011). While standard LDA does not assume the topics to be found a priori, seeded LDA exploits a limited number of words, defined as "seed words," to weigh the prior distribution of topics (identified ex-ante by the researcher according to theoretical considerations) before fitting the model. For this reason, this method is called "semi-supervised."*[^5]

[^5]: Curini, L., & Vignoli, V. (2021). [Committed Moderates and Uncommitted Extremists: Ideological Leaning and Parties' Narratives on Military Interventions in Italy](https://academic.oup.com/fpa/article-abstract/17/3/orab016/6281489). *Foreign Policy Analysis*, 17(3).

------------------------------------------------------------------------

*As a result, seeded models such as seeded LDA stand in the middle between dictionary analysis and unsupervised topic models, presenting advantages with respect to both of them. In comparison with dictionary analysis, they do not require the researcher to compile a long and inevitably debatable list of keywords from scratch*[^6]*.*

[^6]: Ibidem

------------------------------------------------------------------------

*Unlike unsupervised topic models, in which the estimated topics are unlabeled, so that it is up to the researcher to assign these labels by interpreting the content of words most closely associated with each topic (Benoit 2020), a semi-supervised model like seeded LDA is expected to produce results more solid in terms of validity and consistency with the theoretical framework, thanks to the use of the seed words*[^7]*.*

[^7]: Ibidem.

------------------------------------------------------------------------

We first need to provide a small dictionary of keywords (seed words) to define the desired topics. Focusing on the example of extracts from the election manifestos of 9 UK political parties from 2010, related to immigration or asylum-seekers, we might be interested in analyzing a securitarian and an humanitarian frame.

```{r}
imm_frames <- dictionary(list(securitarian = c("control", "border", 
                                               "police", "detention",
                                               "illegal", "legal"),
                        humanitarian = c("asylum", "child", 
                                         "seeker", "refugee",
                                         "human", "right")))

imm_frames
```

------------------------------------------------------------------------

Many of the top terms are seed words but related topic words are also identified.

The topic "other" is a "junk" topic.

```{r}
set.seed(1234)
slda <- textmodel_seededlda(ukimmig2010_dfm, imm_frames, residual = TRUE)
print(terms(slda, 20))
```

```{r}
table(topics(slda))
```

# Hands-on tutorial {background-color="#2780e3"}

Click [here](https://nicolarighetti.github.io/text-analysis-with-R/day2-tutorial.html#/sec-seeded-topic-models) to open the tutorial

# Other Topic Modeling methods

## Biterm topic modelling

The **Biterm Topic Models** has been introduced by Yan et al (2013)[^8] to overcome problems arising from the application of standard topic modeling to short texts such as Tweets.

[^8]: Yan, X., Guo, J., Lan, Y., & Cheng, X. (2013, May). [A biterm topic model for short texts](https://dl.acm.org/doi/10.1145/2488388.2488514). In *Proceedings of the 22nd international conference on World Wide Web* (pp. 1445-1456)

> (...) directly applying conventional topic models (e.g. LDA and PLSA) on such short texts may not work well. The fundamental reason lies in that conventional topic models implicitly capture the document-level word co-occurrence patterns to reveal topics, and thus suffer from the severe data sparsity in short documents.

------------------------------------------------------------------------

> Specifically, in BTM we learn the topics by directly modeling the generation of word co-occurrence patterns (i.e. biterms) in the whole corpus. \[...\] The results demonstrate that our approach can discover more prominent and coherent topics, and significantly outperform baseline methods on several evaluation metrics. Furthermore, we find that BTM can outperform LDA even on normal texts, showing the potential generality and wider usage of the new topic model. (Yan et al., 2013)

------------------------------------------------------------------------

## BTM in R

The R package [BTM](https://cran.r-project.org/package=BTM) can be used to fit biterm topic models and find topics in short texts ([example](https://www.bnosac.be/index.php/blog/98-biterm-topic-modelling-for-short-texts)).

## Topic Modeling in Embedding Spaces

Embedded Topic Modeling (ETM) is a topic modeling method recently developed by Dieng, Ruiz, and Blei (2020)[^9] to overcoming some flawless of classic topic models. The authors explain that:

[^9]: Dieng, A. B., Ruiz, F. J., & Blei, D. M. (2020). [Topic modeling in embedding spaces.](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00325/96463/Topic-Modeling-in-Embedding-Spaces) *Transactions of the Association for Computational Linguistics*, *8*, 439-453.

> (...) *embedded topic model* (etm), \[is\] a generative model of documents that marries traditional topic models with word embeddings. \[...\] The etm discovers interpretable topics even with large vocabularies that include rare words and stop words. It outperforms existing document models (...) in terms of both topic quality and predictive performance.

# Hands-on tutorial {background-color="#2780e3"}

Click [here](https://nicolarighetti.github.io/text-analysis-with-R/day2-tutorial.html#/sec-advanced-topic-modeling-methods) to open the tutorial

# Coffee Break {background-color="#daffad"}

# Laboratory with real-world data {background-color="#2780e3"}

Click [here](https://nicolarighetti.github.io/text-analysis-with-R/day2-tutorial.html#/sec-laboratory-using-real-world-data) to open the tutorial

# Wrap up and key takeaways {background-color="#2780e3"}
